{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fruit Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "\n",
    "# deep learning libraries\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Organize Data: Combine The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will take all the images from the training and test set and put them into one set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all data into one set, the \"Complete_Set\"\n",
    "try:\n",
    "    os.mkdir(\"Complete_Set\")\n",
    "\n",
    "# iterate over train and test sets\n",
    "for set_folder in [\"Training\", \"Test\"]:\n",
    "    \n",
    "    # iterate over all the different folders for fruit in both training and test sets\n",
    "    for fruit_folder in os.listdir(set_folder):\n",
    "        \n",
    "        # create a new folder in the complete set everytime a new fruit is seen\n",
    "        if fruit_folder not in os.listdir(\"Complete_Set\"):\n",
    "            os.mkdir(\"Complete_Set\\\\\" + fruit_folder)\n",
    "        \n",
    "        # iterate over every image \n",
    "        for img in os.listdir(set_folder + \"\\\\\" + fruit_folder):\n",
    "        \n",
    "            # save the names of the old and new file paths\n",
    "            origin = set_folder + \"\\\\\" + fruit_folder + \"\\\\\" + img\n",
    "            destination = \"Complete_Set\\\\\" + fruit_folder + \"\\\\\" + img\n",
    "            \n",
    "            # copy the files into the complete set folder\n",
    "            shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Organize Data: Find Class Distribution\n",
    "Here I will tally up how many pictures belong to each class of fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit name</th>\n",
       "      <th>n_imgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Braeburn</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple Crimson Snow</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple Golden 1</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple Golden 2</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple Golden 3</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fruit name  n_imgs\n",
       "0      Apple Braeburn     656\n",
       "1  Apple Crimson Snow     592\n",
       "2      Apple Golden 1     640\n",
       "3      Apple Golden 2     656\n",
       "4      Apple Golden 3     642"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a storage space for the information\n",
    "fruit_count = []\n",
    "\n",
    "# loop through all the different fruit folders\n",
    "for fruit_folder in os.listdir(\"Complete_Set\"):\n",
    "    \n",
    "    # count the amount of images\n",
    "    n_imgs = len([img for img in os.listdir(\"Complete_Set\\\\\" + fruit_folder)])\n",
    "    \n",
    "    # add data to storage space\n",
    "    fruit_count.append({\"fruit name\":fruit_folder, \"n_imgs\":n_imgs})\n",
    "    \n",
    "# save data to dataframe\n",
    "count_df = pd.DataFrame(fruit_count)\n",
    "\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 131 different types of fruit in this data set. With 90375 images total.\n",
      "\n",
      "The median amount of images per a fruit is 656.0.\n",
      "\n",
      "The least represented fruit is Ginger Root. It has a total of 396 Images.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print how many total fruit their are total and how many images their are total\n",
    "print(f\"There are {len(count_df)} different types of fruit in this data set. With {sum(count_df.n_imgs)} images total.\\n\")\n",
    "\n",
    "# print the median amount of images per a fruit\n",
    "print(f\"The median amount of images per a fruit is {count_df.n_imgs.median()}.\\n\")\n",
    "\n",
    "# print the fruit with the minimum photos and print what that minimum value is\n",
    "rare_fruit = count_df[count_df.n_imgs == count_df.n_imgs.min()][\"fruit name\"].values[0]\n",
    "print(f\"The least represented fruit is {rare_fruit}. It has a total of {count_df.n_imgs.min()} Images.\\n\\n\")\n",
    "\n",
    "# plot distribution of fruit counts\n",
    "plt.hist(count_df.n_imgs)\n",
    "plt.title(\"distribution of the amount of images per a fruit\".title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Organize Data: Train/Validation/Test Split\n",
    "Now I will do a 70/15/15 split into train, validation, and test sets for each unique fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper function for counting images in a fruit folder\n",
    "def img_count(fruit_folder):\n",
    "    n_imgs = len(os.listdir(fruit_folder))\n",
    "    return n_imgs\n",
    "\n",
    "# create a helper function for finding the correct indexes to split on\n",
    "# based on the number of images in a fruit folder\n",
    "def find_indexes(n_imgs):\n",
    "    import numpy as np\n",
    "    \n",
    "    train_index = int(np.ceil(0.7 * n_imgs))\n",
    "    val_index = int(0.85 * n_imgs)\n",
    "    \n",
    "    return train_index, val_index\n",
    "\n",
    "# create a function to split a fruit folder\n",
    "def split_fruit(fruit_folder, train_directory, val_directory, test_directory):\n",
    "    \n",
    "    # find number of images in fruit folder\n",
    "    n_imgs = img_count(\"Complete_Set\\\\\" + fruit_folder)\n",
    "    \n",
    "    # find indexes to split on\n",
    "    train_index, val_index = find_indexes(n_imgs)\n",
    "    \n",
    "    # create a list of all the images in the folder\n",
    "    imgs = [img for img in os.listdir(\"Complete_Set\\\\\" + fruit_folder)]\n",
    "    \n",
    "    # shuffle images to ensure randomness\n",
    "    np.random.shuffle(imgs)\n",
    "    \n",
    "    # iterate over images\n",
    "    for img in imgs:\n",
    "        \n",
    "        # save file path origin for image\n",
    "        origin = \"Complete_Set\\\\\" + fruit_folder + \"\\\\\" + img\n",
    "        \n",
    "        # instantiate a variable for the destination file path\n",
    "        destination = \"\"\n",
    "        \n",
    "        # based on the index find the correct destination\n",
    "        if img in imgs[ : train_index]:\n",
    "            destination = train_directory + \"\\\\\" + fruit_folder +\"\\\\\" + img\n",
    "        elif img in imgs[train_index : val_index]:\n",
    "            destination = val_directory + \"\\\\\" + fruit_folder +\"\\\\\" + img\n",
    "        else:\n",
    "            destination = test_directory + \"\\\\\" + fruit_folder +\"\\\\\" + img\n",
    "            \n",
    "        # copy image to the correct destination\n",
    "        shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all the unique fruits in the complete data set\n",
    "for fruit_folder in os.listdir(\"Complete_Set\"):\n",
    "    \n",
    "    # iterate over the three new sets: train, validation, and test\n",
    "    for subset in [\"new_train\", \"new_validation\", \"new_test\"]:\n",
    "        \n",
    "        # if not done already, create a new directory for each set\n",
    "        try:\n",
    "            os.mkdir(subset)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # if there is not one already, make a new directory for the unique fruit\n",
    "        try:\n",
    "            os.mkdir(subset + \"\\\\\" + fruit_folder)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # copy the images over and do the train/val/test split\n",
    "    split_fruit(fruit_folder, \"new_train\", \"new_validation\", \"new_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Organize Data: Establish X and Y Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from kaggle post \n",
    "# https://www.kaggle.com/aninditapani/cnn-from-scratch-with-98-accuracy\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np.array(data['target'])\n",
    "    target_labels = np.array(data['target_names'])\n",
    "    return files,targets,target_labels\n",
    "    \n",
    "X_train, y_train, target_labels = load_dataset(\"new_train\")\n",
    "X_val, y_val, _ = load_dataset(\"new_validation\")\n",
    "X_test, y_test, _ = load_dataset(\"new_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Preprocessing: One Hot Encode Y Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_y(y):\n",
    "    return np_utils.to_categorical(y, 131)\n",
    "\n",
    "y_train = encode_y(y_train)\n",
    "y_val = encode_y(y_val)\n",
    "y_test = encode_y(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63342, 131)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Preprocessing: Turn X Into A Pixel Value Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new_train\\\\Apple Red 2\\\\320_100.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_to_array(X):\n",
    "    arr = []\n",
    "    for img in X:\n",
    "        arr.append(img_to_array(load_img(img)))\n",
    "    return np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63342, 100, 100, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_to_array(X_train)\n",
    "X_val = X_to_array(X_val)\n",
    "X_test = X_to_array(X_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Preprocessing: Scale X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254., 255., 255.],\n",
       "       [254., 255., 255.],\n",
       "       [254., 255., 255.],\n",
       "       [254., 255., 255.],\n",
       "       [255., 255., 255.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X):\n",
    "    return X\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val = scale(X_val)\n",
    "X_test = scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Modeling: Creating A Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 100, 100, 16)      784       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100, 100, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 25, 25, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 25, 25, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               102600    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 131)               26331     \n",
      "=================================================================\n",
      "Total params: 175,507\n",
      "Trainable params: 175,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inspired from kaggle post \n",
    "# https://www.kaggle.com/aninditapani/cnn-from-scratch-with-98-accuracy\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = 4, input_shape=(100,100,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=4))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 2, activation = 'relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = 2, activation = 'relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(131, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled!\n"
     ]
    }
   ],
   "source": [
    "# taken from kaggle post \n",
    "# https://www.kaggle.com/aninditapani/cnn-from-scratch-with-98-accuracy\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print('Compiled!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Modeling: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63342 samples, validate on 13409 samples\n",
      "Epoch 1/7\n",
      " - 172s - loss: 0.5751 - accuracy: 0.9147 - val_loss: 0.0384 - val_accuracy: 0.9921\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03836, saving model to cnn.hdf5\n",
      "Epoch 2/7\n",
      " - 167s - loss: 0.6416 - accuracy: 0.9102 - val_loss: 0.1337 - val_accuracy: 0.9776\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03836\n",
      "Epoch 3/7\n",
      " - 154s - loss: 0.7071 - accuracy: 0.9085 - val_loss: 0.2205 - val_accuracy: 0.9491\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03836\n",
      "Epoch 4/7\n",
      " - 143s - loss: 0.7139 - accuracy: 0.9042 - val_loss: 0.0433 - val_accuracy: 0.9881\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03836\n",
      "Epoch 5/7\n",
      " - 132s - loss: 0.7510 - accuracy: 0.9063 - val_loss: 0.0345 - val_accuracy: 0.9898\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03836 to 0.03451, saving model to cnn.hdf5\n",
      "Epoch 6/7\n",
      " - 133s - loss: 0.7913 - accuracy: 0.9032 - val_loss: 0.0243 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03451 to 0.02433, saving model to cnn.hdf5\n",
      "Epoch 7/7\n"
     ]
    }
   ],
   "source": [
    "# taken from kaggle post \n",
    "# https://www.kaggle.com/aninditapani/cnn-from-scratch-with-98-accuracy\n",
    "model_checkpoint = ModelCheckpoint(filepath = 'cnn.hdf5', verbose = 1, save_best_only = True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "        batch_size = 50,\n",
    "        epochs = 7,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks = [model_checkpoint],\n",
    "        verbose=2, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
